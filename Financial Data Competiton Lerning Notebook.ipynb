{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4472ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Library\n",
    "# ====================================================\n",
    "import sys\n",
    "import os\n",
    "import gc\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import random\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "import joblib\n",
    "import pickle\n",
    "import itertools\n",
    "from tqdm.auto import tqdm\n",
    "import category_encoders as ce\n",
    "import torch\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, train_test_split, GroupKFold\n",
    "from sklearn.metrics import log_loss, roc_auc_score, matthews_corrcoef, f1_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from catboost import Pool, CatBoostRegressor, CatBoostClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers import Activation\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.models import load_model\n",
    "from keras.callbacks import Callback\n",
    "from keras.models import clone_model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "pd.set_option('display.max_columns',1000)\n",
    "pd.set_option('display.max_rows',100)\n",
    "\n",
    "# ====================================================\n",
    "# Configurations\n",
    "# ====================================================\n",
    "class CFG:\n",
    "    VER = 0\n",
    "    AUTHOR = 'Yuta.K'\n",
    "    COMPETITION = 'FDUA2'\n",
    "    DATA_PATH = Path('/data')\n",
    "    OOF_DATA_PATH = Path('/oof')\n",
    "    MODEL_DATA_PATH = Path('/models')\n",
    "    SUB_DATA_PATH = Path('/submission')\n",
    "    METHOD_LIST = [ 'adaboost','lightgbm', 'xgboost', 'catboost']\n",
    "    seed = 42\n",
    "    n_folds = 7\n",
    "    target_col = 'MIS_Status'\n",
    "    metric = 'f1_score'\n",
    "    metric_maximize_flag = True\n",
    "    num_boost_round = 500\n",
    "    early_stopping_round = 200\n",
    "    verbose = 25\n",
    "    classification_lgb_params = {\n",
    "        'objective': 'binary',\n",
    "        'metric': 'auc',\n",
    "        'learning_rate': 0.05,\n",
    "        'seed': seed,\n",
    "    }\n",
    "    classification_xgb_params = {\n",
    "        'objective': 'binary:logistic',\n",
    "        'eval_metric': 'logloss',\n",
    "        'learning_rate': 0.05,\n",
    "        'random_state': seed,\n",
    "    }\n",
    "    classification_cat_params = {\n",
    "        'learning_rate': 0.05,\n",
    "        'iterations': num_boost_round,\n",
    "        'random_seed': seed,\n",
    "    }\n",
    "    classification_adaboost_params = {\n",
    "        'n_estimators': 100,\n",
    "        'learning_rate': 1.0,\n",
    "        'random_state': 42,\n",
    "    }\n",
    "    \n",
    "# ====================================================\n",
    "# Seed everything\n",
    "# ====================================================\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "seed_everything(CFG.seed)\n",
    "\n",
    "\n",
    "# ====================================================\n",
    "# Metric\n",
    "# ====================================================\n",
    "# f1_score\n",
    "\n",
    "# ====================================================\n",
    "# LightGBM Metric\n",
    "# ====================================================\n",
    "def lgb_metric(y_pred, y_true):\n",
    "    y_true = y_true.get_label()\n",
    "    return 'f1score', f1_score(y_true, np.where(y_pred >= 0.5, 1, 0), average='macro'), CFG.metric_maximize_flag\n",
    "\n",
    "# ====================================================\n",
    "# XGBoost Metric\n",
    "# ====================================================\n",
    "def xgb_metric(y_pred, y_true):\n",
    "    y_true = y_true.get_label()\n",
    "    return 'f1score', f1_score(y_true, np.where(y_pred >= 0.5, 1, 0), average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ed1450",
   "metadata": {},
   "outputs": [],
   "source": [
    "#データの読み込み\n",
    "train_df = pd.read_csv('train.csv', index_col=0)\n",
    "test_df = pd.read_csv('test.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5796270e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Preprocessing(train_df, test_df):\n",
    "    \n",
    "    #欠損値に対する前処理\n",
    "    def deal_missing(input_df):\n",
    "        df = input_df.copy()\n",
    "        for col in ['RevLineCr', 'LowDoc', 'BankState']:\n",
    "            df[col] = input_df[col].fillna('UNK')\n",
    "        for col in ['DisbursementDate','ApprovalDate']:\n",
    "            df[col] = input_df[col].fillna('50-NaN-50')\n",
    "        return df\n",
    "\n",
    "    #金額に対する前処理\n",
    "    def clean_money(input_df):\n",
    "        df = input_df.copy()\n",
    "        for col in ['DisbursementGross', 'GrAppv', 'SBA_Appv']:\n",
    "            df[col] = input_df[col].str[1:].str.replace(',', '').str.replace(' ', '').astype(float)\n",
    "        return df\n",
    "    \n",
    "    #特徴量作成\n",
    "    def make_features(input_df):\n",
    "        df = input_df.copy()\n",
    "        df['NewExist'] = np.where(input_df['NewExist'] == 1, 1, 0)\n",
    "        #日付関係の特徴量作成\n",
    "        df[['DisbursementDay','DisbursementMonth','DisbursementYear']] = df['DisbursementDate'].str.split('-',expand=True)\n",
    "        df[['ApprovalDay','ApprovalMonth','ApprovalYear']] = df['ApprovalDate'].str.split('-',expand=True)\n",
    "        df['DisbursementDay'] = df['DisbursementDay'].astype(int)\n",
    "        df['DisbursementYear'] = df['DisbursementYear'].astype(int)\n",
    "        df['ApprovalDay'] = df['ApprovalDay'].astype(int)\n",
    "        df['ApprovalYear'] = df['ApprovalYear'].astype(int)\n",
    "        Month_dict = {'Jan':1,'Feb':2,'Mar':3,'Apr':4,'May':5,'Jun':6,'Jul':7,'Aug':8,'Sep':9,'Oct':10,'Nov':11,'Dec':12,'NaN':50}\n",
    "        df['DisbursementMonth'] = df['DisbursementMonth'].map(Month_dict)\n",
    "        df['ApprovalMonth'] = df['ApprovalMonth'].map(Month_dict)\n",
    "        df['DisbursementDate'] = df['DisbursementYear'].astype(str)+df['DisbursementMonth'].astype(str)+df['DisbursementDay'].astype(str)\n",
    "        df['DisbursementYear'] = df['DisbursementYear'].apply(lambda x:x - 100 if x >50 else x)\n",
    "        df['ApprovalYear'] = df['ApprovalYear'].apply(lambda x:x - 100 if x >50 else x)\n",
    "        df['CompanyLong'] = df['DisbursementYear'] - df['ApprovalYear']\n",
    "        #破産した米国企業の数を外部データとして入力\n",
    "        #Bankraptcydataの74~80は生成したもので実際の数値ではない。(失業率から換算して生成)\n",
    "        Bankraptcydata={-26:32700,-25:52200,-24:46200,-23:42300,-22:36300,-21:34200,-20:46200,-19:44000,-18:48500,-17:69800,-16:62500,\n",
    "                        -15:64500,-14:72000,-13:81500,-12:83000,-11:64500,-10:65000,-9:67000,-8:71000,-7:67000,-6:58000,-5:51000,\n",
    "                        -4:52500,-3:54000,-2:51000,-1:41000,0:37500,1:35992,2:39845,3:37548,4:36785,5:31952,6:35292,7:21960,8:30741,\n",
    "                        9:49091,10:61148,11:54212,12:46393,13:37552,14:31671,15:26130,16:24797,17:23591,18:23106,19:22157}\n",
    "        #年毎のデータを、1-5年後の平均に変換\n",
    "        datalist = [Bankraptcydata]#年毎の外部データの名前はここに入れる\n",
    "        for k in datalist:\n",
    "            for i in range(len(k)-5):\n",
    "                k[-27+i] = 0\n",
    "                for j in range(5):\n",
    "                    k[-27+i] += k[-26+i+j]\n",
    "                k[-27+i] = k[-27+i]/5\n",
    "            k[50] = k[-26]*2\n",
    "\n",
    "        df['Bankraptcy_By_Year'] = df['DisbursementYear'].map(Bankraptcydata)\n",
    "\n",
    "        #組み合わせ特徴量\n",
    "        df['State_Sector'] = df['State'].astype(str) + '_' + df['Sector'].astype(str)\n",
    "        df['City_State'] = df['City'] + '_' + df['State']\n",
    "        df['ApprovalFY_Term'] = df['ApprovalFY'].astype(str) + '_' + df['Term'].astype(str)\n",
    "        df['FranchiseCode_ApprovalDate'] = df['FranchiseCode'].astype(str) + '_' + df['ApprovalDate'].astype(str)\n",
    "        df['Term_NoEmp'] = df['Term'].astype(str) + '_' + df['NoEmp'].astype(str)\n",
    "        df['City_BankState'] = df['City'].astype(str) + '_' + df['BankState'].astype(str)\n",
    "        df['NoEmp_SBA_Appv'] = df['NoEmp'].astype(str) + '_' + df['SBA_Appv'].astype(str)\n",
    "        return df\n",
    "    \n",
    "    def encoding(train_df, test_df):\n",
    "        #ラベルエンコーディング\n",
    "        categorical_features_for_label = ['RevLineCr', 'LowDoc', 'UrbanRural', 'State', 'Sector',\n",
    "                                'ApprovalFY_Term','City_State','City','ApprovalDate','BankState','DisbursementDate','State_Sector',\n",
    "                                   'FranchiseCode_ApprovalDate','Term_NoEmp','City_BankState','NoEmp_SBA_Appv']\n",
    "        for col in categorical_features_for_label:\n",
    "            encoder = LabelEncoder()\n",
    "            combined = pd.concat([train_df[col], test_df[col]], axis=0)\n",
    "            encoder.fit(combined)\n",
    "            train_df[col] = encoder.transform(train_df[col])\n",
    "            test_df[col] = encoder.transform(test_df[col])\n",
    "        #ワンホットエンコーディング\n",
    "        OneHotList = ['RevLineCr', 'LowDoc']\n",
    "        train_df2 = train_df.drop(['MIS_Status'],axis=1)\n",
    "        ohe = ce.OneHotEncoder(cols=OneHotList,use_cat_names=True)\n",
    "        train_df2 = ohe.fit_transform(train_df2)\n",
    "        test_df = ohe.transform(test_df)\n",
    "        train_df = pd.concat([train_df2,train_df['MIS_Status']],axis=1)\n",
    "        return train_df, test_df\n",
    "\n",
    "    train_df = deal_missing(train_df)\n",
    "    test_df = deal_missing(test_df)\n",
    "    train_df = clean_money(train_df)\n",
    "    test_df = clean_money(test_df)\n",
    "    train_df = make_features(train_df)\n",
    "    test_df = make_features(test_df)\n",
    "    train_df, test_df = encoding(train_df, test_df)\n",
    "    \n",
    "    \n",
    "    return train_df,test_df\n",
    "    \n",
    "#前処理の実行\n",
    "train_df, test_df = Preprocessing(train_df,test_df)\n",
    "\n",
    "#カテゴリカル特徴量の指定\n",
    "categorical_features = ['State', 'Sector','RevLineCr_0.0', 'RevLineCr_1.0', 'RevLineCr_2.0', 'RevLineCr_3.0', 'RevLineCr_4.0',\n",
    "                        'LowDoc_3.0', 'LowDoc_2.0', 'LowDoc_5.0', 'LowDoc_6.0', 'LowDoc_0.0', 'LowDoc_4.0','LowDoc_1.0',\n",
    "                       'ApprovalFY_Term','City_State','City','ApprovalDate','BankState','State_Sector','UrbanRural',\n",
    "                        'FranchiseCode_ApprovalDate','Term_NoEmp','City_BankState','NoEmp_SBA_Appv']\n",
    "\n",
    "#特徴量の指定\n",
    "features = train_df.columns.tolist()\n",
    "#学習に使用しない特徴量は以下で除外\n",
    "RemoveList=['MIS_Status','ApprovalYear']\n",
    "for i in RemoveList:\n",
    "    features.remove(i)\n",
    "print(f'features for training:{features}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3bcf752",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Learning & Predicting\n",
    "\n",
    "#1段階目の学習\n",
    "def Pre_Learning(train_df,test_df, features, categorical_features):\n",
    "    \n",
    "    #adaboostでの学習メソッドの定義\n",
    "    def adaboost_training(x_train, y_train, x_valid, y_valid, features, categorical_features):\n",
    "        model = AdaBoostClassifier(**CFG.classification_adaboost_params)\n",
    "        model.fit(x_train, y_train)\n",
    "        # Predict validation\n",
    "        valid_pred = model.predict_proba(x_valid)[:, 1]\n",
    "        return model, valid_pred\n",
    "\n",
    "    #lightgbmでの学習メソッドの定義\n",
    "    def lightgbm_training(x_train, y_train, x_valid, y_valid, features, categorical_features):\n",
    "        lgb_train = lgb.Dataset(x_train, y_train, categorical_feature=categorical_features)\n",
    "        lgb_valid = lgb.Dataset(x_valid, y_valid, categorical_feature=categorical_features)\n",
    "        model = lgb.train(\n",
    "                    params = CFG.classification_lgb_params,\n",
    "                    train_set = lgb_train,\n",
    "                    num_boost_round = CFG.num_boost_round,\n",
    "                    valid_sets = [lgb_train, lgb_valid],\n",
    "                    feval = lgb_metric,\n",
    "                    callbacks=[lgb.early_stopping(stopping_rounds=CFG.early_stopping_round,\n",
    "                                                  verbose=CFG.verbose)]\n",
    "                )\n",
    "        # Predict validation\n",
    "        valid_pred = model.predict(x_valid)\n",
    "        return model, valid_pred\n",
    "\n",
    "    #xgboostでの学習メソッドの定義\n",
    "    def xgboost_training(x_train, y_train, x_valid, y_valid, features, categorical_features):\n",
    "        xgb_train = xgb.DMatrix(data=x_train, label=y_train)\n",
    "        xgb_valid = xgb.DMatrix(data=x_valid, label=y_valid)\n",
    "        model = xgb.train(\n",
    "                    CFG.classification_xgb_params,\n",
    "                    dtrain = xgb_train,\n",
    "                    num_boost_round = CFG.num_boost_round,\n",
    "                    evals = [(xgb_train, 'train'), (xgb_valid, 'eval')],\n",
    "                    early_stopping_rounds = CFG.early_stopping_round,\n",
    "                    verbose_eval = CFG.verbose,\n",
    "                    feval = xgb_metric,\n",
    "                    maximize = CFG.metric_maximize_flag,\n",
    "                )\n",
    "        # Predict validation\n",
    "        valid_pred = model.predict(xgb.DMatrix(x_valid))\n",
    "        return model, valid_pred\n",
    "\n",
    "    #catboostでの学習メソッドの定義\n",
    "    def catboost_training(x_train, y_train, x_valid, y_valid, features, categorical_features):\n",
    "        cat_train = Pool(data=x_train, label=y_train, cat_features=categorical_features)\n",
    "        cat_valid = Pool(data=x_valid, label=y_valid, cat_features=categorical_features)\n",
    "        model = CatBoostClassifier(**CFG.classification_cat_params)\n",
    "        model.fit(cat_train,\n",
    "                  eval_set = [cat_valid],\n",
    "                  early_stopping_rounds = CFG.early_stopping_round,\n",
    "                  verbose = CFG.verbose,\n",
    "                  use_best_model = True)\n",
    "        # Predict validation\n",
    "        valid_pred = model.predict_proba(x_valid)[:, 1]\n",
    "        return model, valid_pred\n",
    "\n",
    "    #任意のモデルでのクロスバリデーション学習メソッドの定義\n",
    "    def gradient_boosting_model_cv_training(method, train_df, features, categorical_features):\n",
    "        # Create a numpy array to store out of folds predictions\n",
    "        oof_predictions = np.zeros(len(train_df))\n",
    "        oof_fold = np.zeros(len(train_df))\n",
    "        kfold = KFold(n_splits=CFG.n_folds, shuffle=True, random_state=CFG.seed)\n",
    "        for fold, (train_index, valid_index) in enumerate(kfold.split(train_df)):\n",
    "            print('-'*50)\n",
    "            print(f'{method} training fold {fold+1}')\n",
    "\n",
    "            x_train = train_df[features].iloc[train_index]\n",
    "            y_train = train_df[CFG.target_col].iloc[train_index]\n",
    "            x_valid = train_df[features].iloc[valid_index]\n",
    "            y_valid = train_df[CFG.target_col].iloc[valid_index]\n",
    "\n",
    "            model = None  # モデル変数を初期化する\n",
    "            valid_pred = None\n",
    "\n",
    "            if method == 'adaboost':\n",
    "                model, valid_pred = adaboost_training(x_train, y_train, x_valid, y_valid, features, categorical_features)\n",
    "            if method == 'lightgbm':\n",
    "                model, valid_pred = lightgbm_training(x_train, y_train, x_valid, y_valid, features, categorical_features)\n",
    "            if method == 'xgboost':\n",
    "                model, valid_pred = xgboost_training(x_train, y_train, x_valid, y_valid, features, categorical_features)\n",
    "            if method == 'catboost':\n",
    "                model, valid_pred = catboost_training(x_train, y_train, x_valid, y_valid, features, categorical_features)  \n",
    "            # Save best model\n",
    "            pickle.dump(model, open(f'{method}_fold{fold + 1}_seed{CFG.seed}_ver{CFG.VER}.pkl', 'wb'))\n",
    "            # Add to out of folds array\n",
    "            oof_predictions[valid_index] = valid_pred\n",
    "            oof_fold[valid_index] = fold + 1\n",
    "            del x_train, x_valid, y_train, y_valid, model, valid_pred\n",
    "            gc.collect()\n",
    "\n",
    "        # Compute out of folds metric\n",
    "        score = f1_score(train_df[CFG.target_col], oof_predictions >= 0.5, average='macro')\n",
    "        print(f'{method} our out of folds CV f1score is {score}')\n",
    "        # Create a dataframe to store out of folds predictions\n",
    "        oof_df = pd.DataFrame({CFG.target_col: train_df[CFG.target_col], f'{method}_prediction': oof_predictions, 'fold': oof_fold})\n",
    "        oof_df.to_csv(f'oof_{method}_seed{CFG.seed}_ver{CFG.VER}.csv', index = False)\n",
    "\n",
    "    #adaboostの学習済みモデル読み込み関数\n",
    "    def adaboost_inference(x_test):\n",
    "        test_pred = np.zeros(len(x_test))\n",
    "        for fold in range(CFG.n_folds):\n",
    "            model = pickle.load(open(f'adaboost_fold{fold + 1}_seed{CFG.seed}_ver{CFG.VER}.pkl', 'rb'))\n",
    "            # Predict\n",
    "            pred = model.predict_proba(x_test)[:, 1]\n",
    "            test_pred += pred\n",
    "        return test_pred / CFG.n_folds\n",
    "\n",
    "    #lightgbmの学習モデル読み込み関数\n",
    "    def lightgbm_inference(x_test):\n",
    "        test_pred = np.zeros(len(x_test))\n",
    "        for fold in range(CFG.n_folds):\n",
    "            model = pickle.load(open(f'lightgbm_fold{fold + 1}_seed{CFG.seed}_ver{CFG.VER}.pkl', 'rb'))\n",
    "            # Predict\n",
    "            pred = model.predict(x_test)\n",
    "            test_pred += pred\n",
    "        return test_pred / CFG.n_folds\n",
    "\n",
    "    #xgboostの学習モデル読み込み関数\n",
    "    def xgboost_inference(x_test):\n",
    "        test_pred = np.zeros(len(x_test))\n",
    "        for fold in range(CFG.n_folds):\n",
    "            model = pickle.load(open(f'xgboost_fold{fold + 1}_seed{CFG.seed}_ver{CFG.VER}.pkl', 'rb'))\n",
    "            # Predict\n",
    "            pred = model.predict(xgb.DMatrix(x_test))\n",
    "            test_pred += pred\n",
    "        return test_pred / CFG.n_folds\n",
    "\n",
    "    #catboostの学習モデル読み込み関数\n",
    "    def catboost_inference(x_test):\n",
    "        test_pred = np.zeros(len(x_test))\n",
    "        for fold in range(CFG.n_folds):\n",
    "            model = pickle.load(open(f'catboost_fold{fold + 1}_seed{CFG.seed}_ver{CFG.VER}.pkl', 'rb'))\n",
    "            # Predict\n",
    "            pred = model.predict_proba(x_test)[:, 1]\n",
    "            test_pred += pred\n",
    "        return test_pred / CFG.n_folds\n",
    "\n",
    "    #任意のメソッドに対して予測を返す関数\n",
    "    def gradient_boosting_model_inference(method, test_df, features, categorical_features):\n",
    "        x_test = test_df[features]\n",
    "        if method == 'adaboost':\n",
    "            test_pred = adaboost_inference(x_test)\n",
    "        if method == 'lightgbm':\n",
    "            test_pred = lightgbm_inference(x_test)\n",
    "        if method == 'xgboost':\n",
    "            test_pred = xgboost_inference(x_test)\n",
    "        if method == 'catboost':\n",
    "            test_pred = catboost_inference(x_test)\n",
    "        return test_pred\n",
    "\n",
    "    for method in CFG.METHOD_LIST:\n",
    "        gradient_boosting_model_cv_training(method, train_df, features, categorical_features)\n",
    "        test_df[f'{method}_pred_prob'] = gradient_boosting_model_inference(method, test_df, features, categorical_features)\n",
    "        \n",
    "        \n",
    "#2段階目の学習　ニューラルネットワークによるスタッキング\n",
    "def Post_Learning(train_df,test_df):\n",
    "    #ニューラルネットワークモデル作成関数\n",
    "    def create_nn_model(input_shape):\n",
    "        model = Sequential([\n",
    "            Dense(64, input_shape=(input_shape,)),\n",
    "            BatchNormalization(),\n",
    "            Activation('relu'),\n",
    "            Dropout(0.5),\n",
    "\n",
    "            Dense(32),\n",
    "            BatchNormalization(),\n",
    "            Activation('relu'),\n",
    "            Dropout(0.5),\n",
    "\n",
    "            Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "        optimizer = Adam(lr=0.001)\n",
    "        model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        return model\n",
    "\n",
    "    #ニューラルネットワーク用学習スケジューラー\n",
    "    def scheduler(epoch, lr):\n",
    "            if epoch < 10:\n",
    "                return lr\n",
    "            else:\n",
    "                return lr * np.exp(-0.1)\n",
    "\n",
    "    #特徴量同士で積を作る関数\n",
    "    def create_interaction_features(features):\n",
    "            n_features = features.shape[1]\n",
    "            interaction_features = []\n",
    "            for i in range(n_features):\n",
    "                for j in range(i + 1, n_features):\n",
    "                    interaction_features.append(features[:, i] * features[:, j])  \n",
    "            return np.column_stack(interaction_features)\n",
    "    \n",
    "    # OOF予測を基に新たな特徴量を作成\n",
    "    oof_features = np.zeros((train_df.shape[0], len(CFG.METHOD_LIST)))\n",
    "    for i, method in enumerate(CFG.METHOD_LIST):\n",
    "        oof_df = pd.read_csv(f'oof_{method}_seed{CFG.seed}_ver{CFG.VER}.csv')\n",
    "        oof_features[:, i] = oof_df[f'{method}_prediction']\n",
    "    \n",
    "    # テストデータの予測を基に特徴量を作成\n",
    "    test_features = np.zeros((test_df.shape[0], len(CFG.METHOD_LIST)))\n",
    "    for i, method in enumerate(CFG.METHOD_LIST):\n",
    "        test_features[:, i] = test_df[f'{method}_pred_prob']\n",
    "\n",
    "    # 特徴量同士の積を追加\n",
    "    oof_interaction_features = create_interaction_features(oof_features)\n",
    "    test_interaction_features = create_interaction_features(test_features)\n",
    "\n",
    "    # 元の特徴量と相互作用特徴量を組み合わせ\n",
    "    oof_combined_features = np.hstack([oof_features, oof_interaction_features])\n",
    "    test_combined_features = np.hstack([test_features, test_interaction_features])\n",
    "\n",
    "    # 特徴量の標準化\n",
    "    global oof_combined_features_scaled, test_combined_features_scaled\n",
    "    scaler = StandardScaler()\n",
    "    oof_combined_features_scaled = scaler.fit_transform(oof_combined_features)\n",
    "    test_combined_features_scaled = scaler.transform(test_combined_features)   \n",
    "    \n",
    "    # ニューラルネットワークモデルを学習\n",
    "    nn_model = create_nn_model(oof_combined_features_scaled.shape[1])\n",
    "    callbacks_list = [LearningRateScheduler(scheduler)]\n",
    "    nn_model.fit(oof_combined_features_scaled, train_df[CFG.target_col],\n",
    "                 validation_split=0.2, epochs=50, batch_size=32, callbacks=callbacks_list, verbose=1)\n",
    "    nn_model.save(f'nn_stacking_model_seed{CFG.seed}_ver{CFG.VER}.h5')\n",
    "    \n",
    "    #ロジスティック回帰モデルを学習\n",
    "    lr_model = LogisticRegression()\n",
    "    lr_model.fit(oof_combined_features_scaled, train_df[CFG.target_col])\n",
    "    pickle.dump(lr_model, open(f'lr_stacking_model_seed{CFG.seed}_ver{CFG.VER}.pkl','wb'))\n",
    "\n",
    "def Learning_and_Predicting(train_df, test_df, features, categorical_features):\n",
    "    Pre_Learning(train_df, test_df, features, categorical_features)\n",
    "    Post_Learning(train_df, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ee5a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "#学習の実行\n",
    "Learning_and_Predicting(train_df, test_df, features, categorical_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efcca6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Postprocessing\n",
    "\n",
    "def Postprocessing(train_df, test_df):\n",
    "    #最適な閾値を見つける関数\n",
    "    def find_best_threshold_and_score(y_true, y_pred_proba):\n",
    "        best_threshold = 0\n",
    "        best_score = 0\n",
    "        for threshold in np.linspace(0, 1, 1001):\n",
    "            score = f1_score(y_true, y_pred_proba >= threshold, average='macro')\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_threshold = threshold\n",
    "        return best_threshold, best_score\n",
    "    \n",
    "    # ニューラルネットワークモデルの学習データに対する予測確率\n",
    "    nn_model = load_model(f'nn_stacking_model_seed{CFG.seed}_ver{CFG.VER}.h5')\n",
    "    train_pred_proba_nn = nn_model.predict(train_df,oof_combined_features_scaled).flatten()\n",
    "    \n",
    "    # 最適な閾値とスコアを求める\n",
    "    best_threshold_nn, best_score_nn = find_best_threshold_and_score(train_df[CFG.target_col], train_pred_proba_nn)\n",
    "    print(f'NN Best Threshold: {best_threshold_nn}, Best F1 Score: {best_score_nn}')\n",
    "    \n",
    "    # テストデータに対する最終予測\n",
    "    test_pred_proba_nn = nn_model.predict(test_combined_features_scaled).flatten()\n",
    "    test_final_predictions_nn = (test_pred_proba_nn >= best_threshold_nn).astype(int)\n",
    "    # 最終予測結果をコンペ提出用のフォーマットでCSVファイルに出力\n",
    "    submission_df_nn = pd.DataFrame({'Id': test_df.index, 'target': test_final_predictions_nn}).reset_index(drop=True)\n",
    "    submission_df_nn['Id'] = submission_df_nn.index + 4230\n",
    "    submission_df_nn.to_csv(f'stacking_nn_submission_best_score{best_score_nn:.4f}_seed{CFG.seed}_ver{CFG.VER}_{CFG.AUTHOR}.csv', header=False, index=False)\n",
    "    \n",
    "    \n",
    "    # ロジスティック回帰モデルの学習データに対する予測確率\n",
    "    lr_model = pickle.load(open(f'lr_stacking_model_seed{CFG.seed}_ver{CFG.VER}.pkl','rb'))\n",
    "    train_pred_proba_lr = lr_model.predict_proba(oof_combined_features_scaled)[:, 1]\n",
    "    \n",
    "    # 最適な閾値とスコアを求める\n",
    "    best_threshold_lr, best_score_lr = find_best_threshold_and_score(train_df[CFG.target_col], train_pred_proba_nn)\n",
    "    print(f'LR Best Threshold: {best_threshold_lr}, Best F1 Score: {best_score_lr}')\n",
    "    \n",
    "    # テストデータに対する最終予測\n",
    "    test_pred_proba_lr = lr_model.predict_proba(test_combined_features_scaled)[:, 1]\n",
    "    test_final_predictions_lr = (test_pred_proba_lr >= best_threshold_lr).astype(int)\n",
    "    # 最終予測結果をコンペ提出用のフォーマットでCSVファイルに出力\n",
    "    submission_df_lr = pd.DataFrame({'Id': test_df.index, 'target': test_final_predictions_lr}).reset_index(drop=True)\n",
    "    submission_df_lr['Id'] = submission_df_lr.index + 4230\n",
    "    submission_df_lr.to_csv(f'stacking_lr_submission_best_score{best_score_lr:.4f}_seed{CFG.seed}_ver{CFG.VER}_{CFG.AUTHOR}.csv', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af12aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#予測の実行\n",
    "Postprocessing(train_df, test_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
