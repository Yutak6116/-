{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "B3WEPx1JJqlG"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "# ====================================================\n",
    "# Library\n",
    "# ====================================================\n",
    "import os\n",
    "import gc\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import random\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "import joblib\n",
    "import pickle\n",
    "import itertools\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, train_test_split, GroupKFold\n",
    "from sklearn.metrics import log_loss, roc_auc_score, matthews_corrcoef, f1_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import category_encoders as ce\n",
    "import lightgbm as lgb\n",
    "#import optuna.integration.lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from catboost import Pool, CatBoostRegressor, CatBoostClassifier\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import accuracy_score\n",
    "import torch.optim as optim\n",
    "\n",
    "pd.set_option('display.max_columns',1000)\n",
    "pd.set_option('display.max_rows',100)\n",
    "\n",
    "# ====================================================\n",
    "# Configurations\n",
    "# ====================================================\n",
    "class CFG:\n",
    "    VER = 8.1\n",
    "    AUTHOR = 'naokisusami'\n",
    "    COMPETITION = 'FDUA2'\n",
    "    DATA_PATH = Path('/data')\n",
    "    OOF_DATA_PATH = Path('/oof')\n",
    "    MODEL_DATA_PATH = Path('/models')\n",
    "    SUB_DATA_PATH = Path('/submission')\n",
    "    #METHOD_LIST = [ 'neuralnetwork', 'adaboost','lightgbm', 'xgboost', 'catboost']\n",
    "    METHOD_LIST = ['adaboost','lightgbm', 'xgboost', 'catboost']\n",
    "    \n",
    "    seed = 42\n",
    "    n_folds = 7\n",
    "    target_col = 'MIS_Status'\n",
    "    metric = 'f1_score'\n",
    "    metric_maximize_flag = True\n",
    "    num_boost_round = 500\n",
    "    early_stopping_round = 200\n",
    "    verbose = 25\n",
    "    classification_lgb_params = {\n",
    "        'objective': 'binary',\n",
    "        'metric': 'auc',\n",
    "        'learning_rate': 0.05,\n",
    "        'seed': seed,\n",
    "    }\n",
    "    classification_xgb_params = {\n",
    "        'objective': 'binary:logistic',\n",
    "        'eval_metric': 'logloss',\n",
    "        'learning_rate': 0.05,\n",
    "        'random_state': seed,\n",
    "    }\n",
    "\n",
    "    classification_cat_params = {\n",
    "        'learning_rate': 0.05,\n",
    "        'iterations': num_boost_round,\n",
    "        'random_seed': seed,\n",
    "    }\n",
    "    classification_adaboost_params = {\n",
    "        'n_estimators': 100,\n",
    "        'learning_rate': 1.0,\n",
    "        'random_state': seed,\n",
    "    }\n",
    "    nn_params = {\n",
    "        'input_size': 44,  # 特徴量の数に応じて変更してください\n",
    "        'hidden_size': [64, 64],  # 隠れ層のユニット数\n",
    "        'output_size': 1,  # 出力層のユニット数\n",
    "        'dropout_rate': 0.1,\n",
    "        'learning_rate': 0.001,\n",
    "        'batch_size': 64,\n",
    "        'epochs': 10,\n",
    "    }\n",
    "\n",
    "    model_weight_dict = {'lightgbm': 0.30, 'xgboost': 0.10, 'catboost': 0.30, 'adaboost': 0.15, 'neuralnetwork': 0.15}\n",
    "# ====================================================\n",
    "# Seed everything\n",
    "# ====================================================\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "seed_everything(CFG.seed)\n",
    "\n",
    "\n",
    "# ====================================================\n",
    "# Metric\n",
    "# ====================================================\n",
    "# f1_score\n",
    "\n",
    "# ====================================================\n",
    "# LightGBM Metric\n",
    "# ====================================================\n",
    "def lgb_metric(y_pred, y_true):\n",
    "    y_true = y_true.get_label()\n",
    "    return 'f1score', f1_score(y_true, np.where(y_pred >= 0.5, 1, 0), average='macro'), CFG.metric_maximize_flag\n",
    "\n",
    "# ====================================================\n",
    "# XGBoost Metric\n",
    "# ====================================================\n",
    "def xgb_metric(y_pred, y_true):\n",
    "    y_true = y_true.get_label()\n",
    "    return 'f1score', f1_score(y_true, np.where(y_pred >= 0.5, 1, 0), average='macro')\n",
    "\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(cfg['input_size'], cfg['hidden_size'][0])\n",
    "        self.dropout = nn.Dropout(cfg['dropout_rate'])\n",
    "        self.fc2 = nn.Linear(cfg['hidden_size'][0], cfg['hidden_size'][1])\n",
    "        self.fc3 = nn.Linear(cfg['hidden_size'][1], cfg['output_size'])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = torch.sigmoid(self.fc3(x))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "g6R4KoxhL91E"
   },
   "outputs": [],
   "source": [
    "#データの読み込み\n",
    "train_df = pd.read_csv('train.csv', index_col=0)\n",
    "test_df = pd.read_csv('test.csv', index_col=0)\n",
    "categorical_features = ['RevLineCr', 'LowDoc', 'UrbanRural', 'State', 'Sector']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "El2B8eayMmyZ"
   },
   "outputs": [],
   "source": [
    "#前処理メソッドの定義\n",
    "def Preprocessing(input_df: pd.DataFrame()) -> pd.DataFrame():\n",
    "    #欠損値に対する処理\n",
    "    def deal_missing(input_df: pd.DataFrame()) -> pd.DataFrame():\n",
    "        df = input_df.copy()\n",
    "        for col in ['RevLineCr', 'LowDoc', 'BankState']:\n",
    "            df[col] = input_df[col].fillna('UNK')\n",
    "        for col in ['DisbursementDate','ApprovalDate']:\n",
    "            df[col] = input_df[col].fillna('50-NaN-50')\n",
    "        \n",
    "        return df\n",
    "    #金額に対する前処理\n",
    "    def clean_money(input_df: pd.DataFrame()) -> pd.DataFrame():\n",
    "        df = input_df.copy()\n",
    "        for col in ['DisbursementGross', 'GrAppv', 'SBA_Appv']:\n",
    "            df[col] = input_df[col].str[1:].str.replace(',', '').str.replace(' ', '').astype(float)\n",
    "        return df\n",
    "    df = deal_missing(input_df)\n",
    "    df = clean_money(df)\n",
    "    df['NewExist'] = np.where(input_df['NewExist'] == 1, 1, 0)\n",
    "    #特徴量作成\n",
    "    def make_features(input_df: pd.DataFrame()) -> pd.DataFrame():\n",
    "        df = input_df.copy()\n",
    "        #日付関係の特徴量作成\n",
    "        df[['DisbursementDay','DisbursementMonth','DisbursementYear']] = df['DisbursementDate'].str.split('-',expand=True)\n",
    "        df[['ApprovalDay','ApprovalMonth','ApprovalYear']] = df['ApprovalDate'].str.split('-',expand=True)\n",
    "        df['DisbursementDay'] = df['DisbursementDay'].astype(int)\n",
    "        df['DisbursementYear'] = df['DisbursementYear'].astype(int)\n",
    "        df['ApprovalDay'] = df['ApprovalDay'].astype(int)\n",
    "        df['ApprovalYear'] = df['ApprovalYear'].astype(int)\n",
    "        Month_dict = {'Jan':1,'Feb':2,'Mar':3,'Apr':4,'May':5,'Jun':6,'Jul':7,'Aug':8,'Sep':9,'Oct':10,'Nov':11,'Dec':12,'NaN':50}\n",
    "        df['DisbursementMonth'] = df['DisbursementMonth'].map(Month_dict)\n",
    "        df['ApprovalMonth'] = df['ApprovalMonth'].map(Month_dict)\n",
    "        df['DisbursementDate'] = df['DisbursementYear'].astype(str)+df['DisbursementMonth'].astype(str)+df['DisbursementDay'].astype(str)\n",
    "        df['DisbursementYear'] = df['DisbursementYear'].apply(lambda x:x - 100 if x >50 else x)\n",
    "        df['ApprovalYear'] = df['ApprovalYear'].apply(lambda x:x - 100 if x >50 else x)\n",
    "        df['CompanyLong'] = df['DisbursementYear'] - df['ApprovalYear']\n",
    "\n",
    "        #Bankraptcydataの74~80は生成したもので実際の数値ではない。(失業率から換算して生成)\n",
    "        Bankraptcydata={-26:32700,-25:52200,-24:46200,-23:42300,-22:36300,-21:34200,-20:46200,-19:44000,-18:48500,-17:69800,-16:62500,\n",
    "                      -15:64500,-14:72000,-13:81500,-12:83000,-11:64500,-10:65000,-9:67000,-8:71000,-7:67000,-6:58000,-5:51000,\n",
    "                        -4:52500,-3:54000,-2:51000,-1:41000,0:37500,1:35992,2:39845,3:37548,4:36785,5:31952,6:35292,7:21960,8:30741,\n",
    "                        9:49091,10:61148,11:54212,12:46393,13:37552,14:31671,15:26130,16:24797,17:23591,18:23106,19:22157}\n",
    "\n",
    "        #年ごとのデータを、1-5年後の平均に変換\n",
    "        datalist = [Bankraptcydata]\n",
    "        for k in datalist:\n",
    "            for i in range(len(k)-5):\n",
    "                k[-27+i] = 0\n",
    "                for j in range(5):\n",
    "                    k[-27+i] += k[-26+i+j]\n",
    "                k[-27+i] = k[-27+i]/5\n",
    "            k[50] = k[-26]*2\n",
    "        \n",
    "        df['Bankraptcy_By_Year'] = df['DisbursementYear'].map(Bankraptcydata)\n",
    "\n",
    "        #組み合わせ特徴量\n",
    "        df['State_Sector'] = df['State'].astype(str) + '_' + df['Sector'].astype(str)\n",
    "         # 地理的特徴の組み合わせ\n",
    "        df['City_State'] = df['City'] + '_' + df['State']\n",
    "        # 時間的特徴の組み合わせ\n",
    "        df['ApprovalFY_Term'] = df['ApprovalFY'].astype(str) + '_' + df['Term'].astype(str)\n",
    "        \n",
    "        df['FranchiseCode_ApprovalDate'] = df['FranchiseCode'].astype(str) + '_' + df['ApprovalDate'].astype(str)\n",
    "        \n",
    "        df['Term_NoEmp'] = df['Term'].astype(str) + '_' + df['NoEmp'].astype(str)\n",
    "        \n",
    "        df['City_BankState'] = df['City'].astype(str) + '_' + df['BankState'].astype(str)\n",
    "        \n",
    "        df['NoEmp_SBA_Appv'] = df['NoEmp'].astype(str) + '_' + df['SBA_Appv'].astype(str)\n",
    "        \n",
    "\n",
    "\n",
    "        \n",
    "        return df\n",
    "    df = make_features(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "GA0iq9vLNEqS"
   },
   "outputs": [],
   "source": [
    "#前処理の実行\n",
    "train_df = Preprocessing(train_df)\n",
    "test_df = Preprocessing(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 42307 entries, 0 to 42306\n",
      "Data columns (total 35 columns):\n",
      " #   Column                      Non-Null Count  Dtype  \n",
      "---  ------                      --------------  -----  \n",
      " 0   Term                        42307 non-null  int64  \n",
      " 1   NoEmp                       42307 non-null  int64  \n",
      " 2   NewExist                    42307 non-null  int32  \n",
      " 3   CreateJob                   42307 non-null  int64  \n",
      " 4   RetainedJob                 42307 non-null  int64  \n",
      " 5   FranchiseCode               42307 non-null  int64  \n",
      " 6   RevLineCr                   42307 non-null  object \n",
      " 7   LowDoc                      42307 non-null  object \n",
      " 8   DisbursementDate            42307 non-null  object \n",
      " 9   MIS_Status                  42307 non-null  int64  \n",
      " 10  Sector                      42307 non-null  int64  \n",
      " 11  ApprovalDate                42307 non-null  object \n",
      " 12  ApprovalFY                  42307 non-null  int64  \n",
      " 13  City                        42307 non-null  object \n",
      " 14  State                       42307 non-null  object \n",
      " 15  BankState                   42307 non-null  object \n",
      " 16  DisbursementGross           42307 non-null  float64\n",
      " 17  GrAppv                      42307 non-null  float64\n",
      " 18  SBA_Appv                    42307 non-null  float64\n",
      " 19  UrbanRural                  42307 non-null  int64  \n",
      " 20  DisbursementDay             42307 non-null  int32  \n",
      " 21  DisbursementMonth           42307 non-null  int64  \n",
      " 22  DisbursementYear            42307 non-null  int64  \n",
      " 23  ApprovalDay                 42307 non-null  int32  \n",
      " 24  ApprovalMonth               42307 non-null  int64  \n",
      " 25  ApprovalYear                42307 non-null  int64  \n",
      " 26  CompanyLong                 42307 non-null  int64  \n",
      " 27  Bankraptcy_By_Year          42307 non-null  float64\n",
      " 28  State_Sector                42307 non-null  object \n",
      " 29  City_State                  42307 non-null  object \n",
      " 30  ApprovalFY_Term             42307 non-null  object \n",
      " 31  FranchiseCode_ApprovalDate  42307 non-null  object \n",
      " 32  Term_NoEmp                  42307 non-null  object \n",
      " 33  City_BankState              42307 non-null  object \n",
      " 34  NoEmp_SBA_Appv              42307 non-null  object \n",
      "dtypes: float64(4), int32(3), int64(14), object(14)\n",
      "memory usage: 11.1+ MB\n"
     ]
    }
   ],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0dJ6ijFuQoF5"
   },
   "source": [
    "（以下はPreprocessingに本来組み込むべきだが，コードが煩雑になるので，いったん切り出している．）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "wFYHaRqrOfGG"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "#カウントエンコーディング\n",
    "for col in categorical_features:\n",
    "    count_dict = dict(train_df[col].value_counts())\n",
    "    train_df[f'{col}_count_encoding'] = train_df[col].map(count_dict).astype(int)\n",
    "    test_df[f'{col}_count_encoding'] = test_df[col].map(count_dict).fillna(1).astype(int)\n",
    "'''\n",
    "\n",
    "#ラベルエンコーディング\n",
    "for col in categorical_features :\n",
    "    le = LabelEncoder()\n",
    "    le.fit(train_df[col])\n",
    "    train_df[col] = le.transform(train_df[col])\n",
    "    test_df[col] = le.transform(test_df[col])\n",
    "    \n",
    "categorical_features_unlabelable = ['ApprovalFY_Term','City_State','City','ApprovalDate','BankState','DisbursementDate','State_Sector',\n",
    "                                   'FranchiseCode_ApprovalDate','Term_NoEmp','City_BankState','NoEmp_SBA_Appv']\n",
    "'''\n",
    "for col in categorical_features_unlabelable:\n",
    "    le = LabelEncoder()   \n",
    "    le.fit(train_df[col])\n",
    "    train_df[col] = le.transform(train_df[col])\n",
    "    test_df[col] = test_df[col].apply(lambda x: le.transform([x])[0] if x in le.classes_ else len(le.classes_))\n",
    "'''\n",
    "for col in categorical_features_unlabelable:\n",
    "    encoder = LabelEncoder()\n",
    "    combined = pd.concat([train_df[col], test_df[col]], axis=0)\n",
    "    encoder.fit(combined)\n",
    "    train_df[col] = encoder.transform(train_df[col])\n",
    "    test_df[col] = encoder.transform(test_df[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 42307 entries, 0 to 42306\n",
      "Data columns (total 35 columns):\n",
      " #   Column                      Non-Null Count  Dtype  \n",
      "---  ------                      --------------  -----  \n",
      " 0   Term                        42307 non-null  int64  \n",
      " 1   NoEmp                       42307 non-null  int64  \n",
      " 2   NewExist                    42307 non-null  int32  \n",
      " 3   CreateJob                   42307 non-null  int64  \n",
      " 4   RetainedJob                 42307 non-null  int64  \n",
      " 5   FranchiseCode               42307 non-null  int64  \n",
      " 6   RevLineCr                   42307 non-null  int32  \n",
      " 7   LowDoc                      42307 non-null  int32  \n",
      " 8   DisbursementDate            42307 non-null  int32  \n",
      " 9   MIS_Status                  42307 non-null  int64  \n",
      " 10  Sector                      42307 non-null  int64  \n",
      " 11  ApprovalDate                42307 non-null  int32  \n",
      " 12  ApprovalFY                  42307 non-null  int64  \n",
      " 13  City                        42307 non-null  int32  \n",
      " 14  State                       42307 non-null  int32  \n",
      " 15  BankState                   42307 non-null  int32  \n",
      " 16  DisbursementGross           42307 non-null  float64\n",
      " 17  GrAppv                      42307 non-null  float64\n",
      " 18  SBA_Appv                    42307 non-null  float64\n",
      " 19  UrbanRural                  42307 non-null  int64  \n",
      " 20  DisbursementDay             42307 non-null  int32  \n",
      " 21  DisbursementMonth           42307 non-null  int64  \n",
      " 22  DisbursementYear            42307 non-null  int64  \n",
      " 23  ApprovalDay                 42307 non-null  int32  \n",
      " 24  ApprovalMonth               42307 non-null  int64  \n",
      " 25  ApprovalYear                42307 non-null  int64  \n",
      " 26  CompanyLong                 42307 non-null  int64  \n",
      " 27  Bankraptcy_By_Year          42307 non-null  float64\n",
      " 28  State_Sector                42307 non-null  int32  \n",
      " 29  City_State                  42307 non-null  int32  \n",
      " 30  ApprovalFY_Term             42307 non-null  int32  \n",
      " 31  FranchiseCode_ApprovalDate  42307 non-null  int32  \n",
      " 32  Term_NoEmp                  42307 non-null  int32  \n",
      " 33  City_BankState              42307 non-null  int32  \n",
      " 34  NoEmp_SBA_Appv              42307 non-null  int32  \n",
      "dtypes: float64(4), int32(17), int64(14)\n",
      "memory usage: 8.9 MB\n"
     ]
    }
   ],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MIS_Status\n"
     ]
    }
   ],
   "source": [
    "#OneHotEncoding\n",
    "train_df2 = train_df.drop(['MIS_Status'],axis=1)\n",
    "OneHotList = ['RevLineCr', 'LowDoc']\n",
    "ohe = ce.OneHotEncoder(cols=OneHotList,use_cat_names=True)\n",
    "train_df2 = ohe.fit_transform(train_df2)\n",
    "test_df = ohe.transform(test_df)\n",
    "train_df = pd.concat([train_df2,train_df['MIS_Status']],axis=1)\n",
    "\n",
    "#featuresの作成\n",
    "categorical_features = ['State', 'Sector','RevLineCr_0.0','RevLineCr_1.0','RevLineCr_2.0','RevLineCr_3.0','RevLineCr_4.0',\n",
    "                       'LowDoc_0.0','LowDoc_1.0','LowDoc_2.0','LowDoc_3.0','LowDoc_4.0','LowDoc_5.0','LowDoc_6.0',\n",
    "                       'ApprovalFY_Term','City_State','City','ApprovalDate','BankState','State_Sector','UrbanRural',\n",
    "                        'FranchiseCode_ApprovalDate','Term_NoEmp','City_BankState','NoEmp_SBA_Appv']\n",
    "\n",
    "\n",
    "RemoveList=['MIS_Status']\n",
    "features = train_df.columns.tolist()\n",
    "for i in RemoveList:\n",
    "    print(i)\n",
    "    features.remove(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Term  NoEmp  NewExist  CreateJob  RetainedJob  FranchiseCode  \\\n",
      "0       163     21         1          0            0              1   \n",
      "1        84      6         1          4            0              0   \n",
      "2       242     45         1          4           90              0   \n",
      "3       237      4         1          0            0              0   \n",
      "4       184      0         1          0            0              0   \n",
      "...     ...    ...       ...        ...          ...            ...   \n",
      "42302   283     14         1          0            0              1   \n",
      "42303    53      2         1          0            0              0   \n",
      "42304    59      6         0          0            0              1   \n",
      "42305   295     18         1          0            8              0   \n",
      "42306    84      4         1          0            8              0   \n",
      "\n",
      "       RevLineCr_1.0  RevLineCr_0.0  RevLineCr_4.0  RevLineCr_3.0  \\\n",
      "0                  1              0              0              0   \n",
      "1                  0              1              0              0   \n",
      "2                  1              0              0              0   \n",
      "3                  1              0              0              0   \n",
      "4                  1              0              0              0   \n",
      "...              ...            ...            ...            ...   \n",
      "42302              1              0              0              0   \n",
      "42303              0              0              1              0   \n",
      "42304              1              0              0              0   \n",
      "42305              1              0              0              0   \n",
      "42306              1              0              0              0   \n",
      "\n",
      "       RevLineCr_2.0  LowDoc_3.0  LowDoc_2.0  LowDoc_5.0  LowDoc_6.0  \\\n",
      "0                  0           1           0           0           0   \n",
      "1                  0           1           0           0           0   \n",
      "2                  0           1           0           0           0   \n",
      "3                  0           1           0           0           0   \n",
      "4                  0           1           0           0           0   \n",
      "...              ...         ...         ...         ...         ...   \n",
      "42302              0           1           0           0           0   \n",
      "42303              0           1           0           0           0   \n",
      "42304              0           1           0           0           0   \n",
      "42305              0           1           0           0           0   \n",
      "42306              0           1           0           0           0   \n",
      "\n",
      "       LowDoc_0.0  LowDoc_4.0  LowDoc_1.0  DisbursementDate  Sector  \\\n",
      "0               0           0           0               847       0   \n",
      "1               0           0           0               638      20   \n",
      "2               0           0           0               232       8   \n",
      "3               0           0           0               386       7   \n",
      "4               0           0           0               427       0   \n",
      "...           ...         ...         ...               ...     ...   \n",
      "42302           0           0           0               847       0   \n",
      "42303           0           0           0               605       8   \n",
      "42304           0           0           0               270       8   \n",
      "42305           0           0           0               800       8   \n",
      "42306           0           0           0               534      22   \n",
      "\n",
      "       ApprovalDate  ApprovalFY  City  State  BankState  DisbursementGross  \\\n",
      "0              2084        2006  2208      3         42            80000.0   \n",
      "1              3265        1992  1723     36         36           287000.0   \n",
      "2              1232        2001  1214     31         31            31983.0   \n",
      "3              3793        2004  1906     42         42           229000.0   \n",
      "4              1126        2000  2246      4          4           525000.0   \n",
      "...             ...         ...   ...    ...        ...                ...   \n",
      "42302          1603        1995  2207     38         38            80000.0   \n",
      "42303          3756        2007  1594      4         42             5000.0   \n",
      "42304           747        2003   580     35         35            60000.0   \n",
      "42305          2117        1989   547     23         23           294000.0   \n",
      "42306           379        2011  2489      4         27            67500.0   \n",
      "\n",
      "         GrAppv  SBA_Appv  UrbanRural  DisbursementDay  DisbursementMonth  \\\n",
      "0       80000.0   68000.0           0               31                  1   \n",
      "1      287000.0  229600.0           0               31                 10   \n",
      "2       30000.0   15000.0           1               31                  8   \n",
      "3      229000.0  229000.0           0               31                  8   \n",
      "4      525000.0  393750.0           0                8                  6   \n",
      "...         ...       ...         ...              ...                ...   \n",
      "42302   80000.0   68000.0           0               31                  1   \n",
      "42303    5000.0    4250.0           1                3                  4   \n",
      "42304   60000.0   51000.0           0               28                  2   \n",
      "42305  294000.0  220500.0           0               10                 12   \n",
      "42306   67500.0   50625.0           0               31                 10   \n",
      "\n",
      "       DisbursementYear  ApprovalDay  ApprovalMonth  ApprovalYear  \\\n",
      "0                    -2           22              9             6   \n",
      "1                    -7           30              6            -8   \n",
      "2                     1           18              4             1   \n",
      "3                     7            6             10             3   \n",
      "4                   -17           17             12            -1   \n",
      "...                 ...          ...            ...           ...   \n",
      "42302                -2            2              3            -5   \n",
      "42303                -9            6              6             7   \n",
      "42304                 3           14              3             3   \n",
      "42305                -3           23              8           -11   \n",
      "42306               -11           12              4            11   \n",
      "\n",
      "       CompanyLong  Bankraptcy_By_Year  State_Sector  City_State  \\\n",
      "0               -8             38377.0            59        2578   \n",
      "1                1             53300.0           746        2008   \n",
      "2                0             36284.4           632        1406   \n",
      "3                4             48317.0           857        2231   \n",
      "4              -16             72700.0            81        2624   \n",
      "...            ...                 ...           ...         ...   \n",
      "42302            3             38377.0           772        2576   \n",
      "42303          -16             59900.0            89        1851   \n",
      "42304            0             31346.0           716         682   \n",
      "42305            8             41067.4           477         640   \n",
      "42306          -22             65600.0           103        2899   \n",
      "\n",
      "       ApprovalFY_Term  FranchiseCode_ApprovalDate  Term_NoEmp  \\\n",
      "0                 3645                        5579         970   \n",
      "1                 1201                        3228        7072   \n",
      "2                 2707                        1218        2728   \n",
      "3                 3294                        3746        2320   \n",
      "4                 2476                        1114        1683   \n",
      "...                ...                         ...         ...   \n",
      "42302             1612                        5331        3185   \n",
      "42303             3961                        3710        5183   \n",
      "42304             3170                        4894        5608   \n",
      "42305              708                        2095        3598   \n",
      "42306             4670                         375        7050   \n",
      "\n",
      "       City_BankState  NoEmp_SBA_Appv  MIS_Status  \n",
      "0                3461            7616           1  \n",
      "1                2696           16401           1  \n",
      "2                1882           12001           1  \n",
      "3                2983           13073           1  \n",
      "4                3530             225           1  \n",
      "...               ...             ...         ...  \n",
      "42302            3449            2792           1  \n",
      "42303            2506            9061           1  \n",
      "42304             934           16688           1  \n",
      "42305             875            5347           1  \n",
      "42306            3926           13526           1  \n",
      "\n",
      "[42307 rows x 45 columns]\n",
      "['Term', 'NoEmp', 'NewExist', 'CreateJob', 'RetainedJob', 'FranchiseCode', 'RevLineCr_1.0', 'RevLineCr_0.0', 'RevLineCr_4.0', 'RevLineCr_3.0', 'RevLineCr_2.0', 'LowDoc_3.0', 'LowDoc_2.0', 'LowDoc_5.0', 'LowDoc_6.0', 'LowDoc_0.0', 'LowDoc_4.0', 'LowDoc_1.0', 'DisbursementDate', 'Sector', 'ApprovalDate', 'ApprovalFY', 'City', 'State', 'BankState', 'DisbursementGross', 'GrAppv', 'SBA_Appv', 'UrbanRural', 'DisbursementDay', 'DisbursementMonth', 'DisbursementYear', 'ApprovalDay', 'ApprovalMonth', 'ApprovalYear', 'CompanyLong', 'Bankraptcy_By_Year', 'State_Sector', 'City_State', 'ApprovalFY_Term', 'FranchiseCode_ApprovalDate', 'Term_NoEmp', 'City_BankState', 'NoEmp_SBA_Appv']\n"
     ]
    }
   ],
   "source": [
    "print(train_df)\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "quaQcTgQOjyJ"
   },
   "outputs": [],
   "source": [
    "# AdaBoost training\n",
    "def adaboost_training(x_train: pd.DataFrame, y_train: pd.DataFrame, x_valid: pd.DataFrame, y_valid: pd.DataFrame, features: list, categorical_features: list):\n",
    "    model = AdaBoostClassifier(**CFG.classification_adaboost_params)\n",
    "    model.fit(x_train, y_train)\n",
    "    valid_pred = model.predict_proba(x_valid)[:, 1]\n",
    "    return model, valid_pred\n",
    "\n",
    "\n",
    "def nn_training(x_train: pd.DataFrame, y_train: pd.DataFrame, x_valid: pd.DataFrame, y_valid: pd.DataFrame, features: list, categorical_features: list):\n",
    "    stdscl = StandardScaler()\n",
    "    x_train_scaled = stdscl.fit_transform(x_train[features])\n",
    "    x_valid_scaled = stdscl.transform(x_valid[features])\n",
    "    \n",
    "    # データをテンソルに変換\n",
    "    x_train_tensor = torch.tensor(x_train_scaled, dtype=torch.float32)\n",
    "    y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).view(-1, 1)\n",
    "    x_valid_tensor = torch.tensor(x_valid_scaled, dtype=torch.float32)\n",
    "    \n",
    "    train_dataset = TensorDataset(x_train_tensor, y_train_tensor)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=CFG.nn_params['batch_size'], shuffle=True)\n",
    "    \n",
    "    model = SimpleNN(CFG.nn_params)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=CFG.nn_params['learning_rate'])\n",
    "    class_counts = y_train.value_counts()\n",
    "    majority_class_count = class_counts.max()\n",
    "    pos_weight = torch.tensor([majority_class_count / class_counts.min()], dtype=torch.float32)\n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "    \n",
    "    model.train()\n",
    "    for epoch in range(CFG.nn_params['epochs']):\n",
    "        for data, target in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "    # バリデーションデータで予測\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        valid_pred = torch.sigmoid(model(x_valid_tensor)).numpy().reshape(-1)\n",
    "    \n",
    "    return model, valid_pred\n",
    "\n",
    "\n",
    "#lightgbmでの学習メソッドの定義\n",
    "def lightgbm_training(x_train: pd.DataFrame, y_train: pd.DataFrame, x_valid: pd.DataFrame, y_valid: pd.DataFrame, features: list, categorical_features: list):\n",
    "    lgb_train = lgb.Dataset(x_train, y_train, categorical_feature=categorical_features)\n",
    "    lgb_valid = lgb.Dataset(x_valid, y_valid, categorical_feature=categorical_features)\n",
    "    model = lgb.train(\n",
    "                params = CFG.classification_lgb_params,\n",
    "                train_set = lgb_train,\n",
    "                num_boost_round = CFG.num_boost_round,\n",
    "                valid_sets = [lgb_train, lgb_valid],\n",
    "                feval = lgb_metric,\n",
    "                callbacks=[lgb.early_stopping(stopping_rounds=CFG.early_stopping_round,\n",
    "                                              verbose=CFG.verbose)]\n",
    "            )\n",
    "    # Predict validation\n",
    "    valid_pred = model.predict(x_valid)\n",
    "    return model, valid_pred\n",
    "\n",
    "#xgboostでの学習メソッドの定義\n",
    "def xgboost_training(x_train: pd.DataFrame, y_train: pd.DataFrame, x_valid: pd.DataFrame, y_valid: pd.DataFrame, features: list, categorical_features: list):\n",
    "    xgb_train = xgb.DMatrix(data=x_train, label=y_train)\n",
    "    xgb_valid = xgb.DMatrix(data=x_valid, label=y_valid)\n",
    "    model = xgb.train(\n",
    "                CFG.classification_xgb_params,\n",
    "                dtrain = xgb_train,\n",
    "                num_boost_round = CFG.num_boost_round,\n",
    "                evals = [(xgb_train, 'train'), (xgb_valid, 'eval')],\n",
    "                early_stopping_rounds = CFG.early_stopping_round,\n",
    "                verbose_eval = CFG.verbose,\n",
    "                feval = xgb_metric,\n",
    "                maximize = CFG.metric_maximize_flag,\n",
    "            )\n",
    "    # Predict validation\n",
    "    valid_pred = model.predict(xgb.DMatrix(x_valid))\n",
    "    return model, valid_pred\n",
    "\n",
    "#catboostでの学習メソッドの定義\n",
    "def catboost_training(x_train: pd.DataFrame, y_train: pd.DataFrame, x_valid: pd.DataFrame, y_valid: pd.DataFrame, features: list, categorical_features: list):\n",
    "    cat_train = Pool(data=x_train, label=y_train, cat_features=categorical_features)\n",
    "    cat_valid = Pool(data=x_valid, label=y_valid, cat_features=categorical_features)\n",
    "    model = CatBoostClassifier(**CFG.classification_cat_params)\n",
    "    model.fit(cat_train,\n",
    "              eval_set = [cat_valid],\n",
    "              early_stopping_rounds = CFG.early_stopping_round,\n",
    "              verbose = CFG.verbose,\n",
    "              use_best_model = True)\n",
    "    # Predict validation\n",
    "    valid_pred = model.predict_proba(x_valid)[:, 1]\n",
    "    return model, valid_pred\n",
    "\n",
    "    \n",
    "                    \n",
    "#任意のモデルでのクロスバリデーション学習メソッドの定義\n",
    "def gradient_boosting_model_cv_training(method: str, train_df: pd.DataFrame, features: list, categorical_features: list):\n",
    "    # Create a numpy array to store out of folds predictions\n",
    "    oof_predictions = np.zeros(len(train_df))\n",
    "    oof_fold = np.zeros(len(train_df))\n",
    "    kfold = KFold(n_splits=CFG.n_folds, shuffle=True, random_state=CFG.seed)\n",
    "    for fold, (train_index, valid_index) in enumerate(kfold.split(train_df)):\n",
    "        print('-'*50)\n",
    "        print(f'{method} training fold {fold+1}')\n",
    "\n",
    "        x_train = train_df[features].iloc[train_index]\n",
    "        y_train = train_df[CFG.target_col].iloc[train_index]\n",
    "        x_valid = train_df[features].iloc[valid_index]\n",
    "        y_valid = train_df[CFG.target_col].iloc[valid_index]\n",
    "        \n",
    "        model = None  # モデル変数を初期化する\n",
    "        valid_pred = None\n",
    "        \n",
    "        if method == 'adaboost':\n",
    "            model, valid_pred = adaboost_training(x_train, y_train, x_valid, y_valid, features, categorical_features)\n",
    "        if method == 'neuralnetwork':\n",
    "            model, valid_pred = nn_training(x_train, y_train, x_valid, y_valid, features, categorical_features)\n",
    "        if method == 'lightgbm':\n",
    "            model, valid_pred = lightgbm_training(x_train, y_train, x_valid, y_valid, features, categorical_features)\n",
    "        if method == 'xgboost':\n",
    "            model, valid_pred = xgboost_training(x_train, y_train, x_valid, y_valid, features, categorical_features)\n",
    "        if method == 'catboost':\n",
    "            model, valid_pred = catboost_training(x_train, y_train, x_valid, y_valid, features, categorical_features)\n",
    "        # Save best model\n",
    "        pickle.dump(model, open(f'{method}_fold{fold + 1}_seed{CFG.seed}_ver{CFG.VER}.pkl', 'wb'))\n",
    "        # Add to out of folds array\n",
    "        oof_predictions[valid_index] = valid_pred\n",
    "        oof_fold[valid_index] = fold + 1\n",
    "        del x_train, x_valid, y_train, y_valid, model, valid_pred\n",
    "        gc.collect()\n",
    "\n",
    "    # Compute out of folds metric\n",
    "    score = f1_score(train_df[CFG.target_col], oof_predictions >= 0.5, average='macro')\n",
    "    print(f'{method} our out of folds CV f1score is {score}')\n",
    "    # Create a dataframe to store out of folds predictions\n",
    "    oof_df = pd.DataFrame({CFG.target_col: train_df[CFG.target_col], f'{method}_prediction': oof_predictions, 'fold': oof_fold})\n",
    "    oof_df.to_csv(f'oof_{method}_seed{CFG.seed}_ver{CFG.VER}.csv', index = False)\n",
    "#学習メソッドの定義\n",
    "def Learning(input_df: pd.DataFrame, features: list, categorical_features: list):\n",
    "    for method in CFG.METHOD_LIST:\n",
    "        gradient_boosting_model_cv_training(method, input_df, features, categorical_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kWzQv798OiQ-",
    "outputId": "57cabf2c-5c42-4084-e263-e00eaeb695ec",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "adaboost training fold 1\n",
      "--------------------------------------------------\n",
      "adaboost training fold 2\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_9192/691614812.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mLearning\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcategorical_features\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_9192/3444676559.py\u001b[0m in \u001b[0;36mLearning\u001b[1;34m(input_df, features, categorical_features)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mLearning\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_df\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcategorical_features\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mmethod\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mCFG\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMETHOD_LIST\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 142\u001b[1;33m         \u001b[0mgradient_boosting_model_cv_training\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcategorical_features\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_9192/3444676559.py\u001b[0m in \u001b[0;36mgradient_boosting_model_cv_training\u001b[1;34m(method, train_df, features, categorical_features)\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'adaboost'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 116\u001b[1;33m             \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0madaboost_training\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_valid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_valid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcategorical_features\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    117\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'neuralnetwork'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn_training\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_valid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_valid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcategorical_features\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_9192/3444676559.py\u001b[0m in \u001b[0;36madaboost_training\u001b[1;34m(x_train, y_train, x_valid, y_valid, features, categorical_features)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0madaboost_training\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_valid\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_valid\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcategorical_features\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAdaBoostClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mCFG\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclassification_adaboost_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mvalid_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_valid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_pred\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    441\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    442\u001b[0m         \u001b[1;31m# Fit\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 443\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    444\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    445\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_validate_estimator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    128\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0miboost\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m             \u001b[1;31m# Boosting step\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 130\u001b[1;33m             sample_weight, estimator_weight, estimator_error = self._boost(\n\u001b[0m\u001b[0;32m    131\u001b[0m                 \u001b[0miboost\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m                 \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py\u001b[0m in \u001b[0;36m_boost\u001b[1;34m(self, iboost, X, y, sample_weight, random_state)\u001b[0m\n\u001b[0;32m    501\u001b[0m         \"\"\"\n\u001b[0;32m    502\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0malgorithm\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'SAMME.R'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 503\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_boost_real\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miboost\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    504\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    505\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# elif self.algorithm == \"SAMME\":\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py\u001b[0m in \u001b[0;36m_boost_real\u001b[1;34m(self, iboost, X, y, sample_weight, random_state)\u001b[0m\n\u001b[0;32m    511\u001b[0m         \u001b[0mestimator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_estimator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 513\u001b[1;33m         \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    514\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    515\u001b[0m         \u001b[0my_predict_proba\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m    901\u001b[0m         \"\"\"\n\u001b[0;32m    902\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 903\u001b[1;33m         super().fit(\n\u001b[0m\u001b[0;32m    904\u001b[0m             \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    905\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m    392\u001b[0m                                            min_impurity_split)\n\u001b[0;32m    393\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 394\u001b[1;33m         \u001b[0mbuilder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    395\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    396\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mis_classifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "Learning(train_df, features, categorical_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PcFYu2WqOzxS"
   },
   "outputs": [],
   "source": [
    "def adaboost_inference(x_test):\n",
    "    test_pred = np.zeros(len(x_test))\n",
    "    for fold in range(CFG.n_folds):\n",
    "        model = pickle.load(open(f'adaboost_fold{fold + 1}_seed{CFG.seed}_ver{CFG.VER}.pkl', 'rb'))\n",
    "        # Predict\n",
    "        pred = model.predict_proba(x_test)[:, 1]\n",
    "        test_pred += pred\n",
    "    return test_pred / CFG.n_folds\n",
    "\n",
    "def nn_inference( x_test: pd.DataFrame):\n",
    "    stdscl = StandardScaler()\n",
    "    x_test = stdscl.fit_transform(x_test)\n",
    "    x_test_tensor = torch.tensor(x_test, dtype=torch.float32)\n",
    "    test_pred = np.zeros((x_test.shape[0],))\n",
    "    for fold in range(CFG.n_folds):\n",
    "        model = pickle.load(open(f'neuralnetwork_fold{fold + 1}_seed{CFG.seed}_ver{CFG.VER}.pkl', 'rb'))\n",
    "        # Predict\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            pred = torch.sigmoid(model(x_test_tensor)).numpy().reshape(-1)\n",
    "            test_pred += pred\n",
    "    return test_pred / CFG.n_folds\n",
    "\n",
    "def lightgbm_inference(x_test: pd.DataFrame):\n",
    "    test_pred = np.zeros(len(x_test))\n",
    "    for fold in range(CFG.n_folds):\n",
    "        model = pickle.load(open(f'lightgbm_fold{fold + 1}_seed{CFG.seed}_ver{CFG.VER}.pkl', 'rb'))\n",
    "        # Predict\n",
    "        pred = model.predict(x_test)\n",
    "        test_pred += pred\n",
    "    return test_pred / CFG.n_folds\n",
    "\n",
    "def xgboost_inference(x_test: pd.DataFrame):\n",
    "    test_pred = np.zeros(len(x_test))\n",
    "    for fold in range(CFG.n_folds):\n",
    "        model = pickle.load(open(f'xgboost_fold{fold + 1}_seed{CFG.seed}_ver{CFG.VER}.pkl', 'rb'))\n",
    "        # Predict\n",
    "        pred = model.predict(xgb.DMatrix(x_test))\n",
    "        test_pred += pred\n",
    "    return test_pred / CFG.n_folds\n",
    "\n",
    "def catboost_inference(x_test: pd.DataFrame):\n",
    "    test_pred = np.zeros(len(x_test))\n",
    "    for fold in range(CFG.n_folds):\n",
    "        model = pickle.load(open(f'catboost_fold{fold + 1}_seed{CFG.seed}_ver{CFG.VER}.pkl', 'rb'))\n",
    "        # Predict\n",
    "        pred = model.predict_proba(x_test)[:, 1]\n",
    "        test_pred += pred\n",
    "    return test_pred / CFG.n_folds\n",
    "\n",
    "def gradient_boosting_model_inference(method: str, test_df: pd.DataFrame, features: list, categorical_features: list):\n",
    "    x_test = test_df[features]\n",
    "    if method == 'adaboost':\n",
    "        test_pred = adaboost_inference(x_test)\n",
    "    if method == 'neuralnetwork':\n",
    "        test_pred = nn_inference(x_test)\n",
    "    if method == 'lightgbm':\n",
    "        test_pred = lightgbm_inference(x_test)\n",
    "    if method == 'xgboost':\n",
    "        test_pred = xgboost_inference(x_test)\n",
    "    if method == 'catboost':\n",
    "        test_pred = catboost_inference(x_test)\n",
    "    return test_pred\n",
    "\n",
    "def Predicting(input_df: pd.DataFrame, features: list, categorical_features: list):\n",
    "    output_df = input_df.copy()\n",
    "    output_df['pred_prob'] = 0\n",
    "    for method in CFG.METHOD_LIST:\n",
    "        output_df[f'{method}_pred_prob'] = gradient_boosting_model_inference(method, input_df, features, categorical_features)\n",
    "        output_df['pred_prob'] += CFG.model_weight_dict[method] * output_df[f'{method}_pred_prob']\n",
    "    return output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mGMj6OftPMpC"
   },
   "outputs": [],
   "source": [
    "test_df = Predicting(test_df, features, categorical_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k11-nGPEZS1h"
   },
   "outputs": [],
   "source": [
    "#後処理の定義\n",
    "def Postprocessing(train_df: pd.DataFrame(), test_df: pd.DataFrame()) -> (pd.DataFrame(), pd.DataFrame()):\n",
    "    train_df['pred_prob'] = 0\n",
    "    for method in CFG.METHOD_LIST:\n",
    "        oof_df = pd.read_csv(f'oof_{method}_seed{CFG.seed}_ver{CFG.VER}.csv')\n",
    "        train_df['pred_prob'] += CFG.model_weight_dict[method] * oof_df[f'{method}_prediction']\n",
    "    best_score = 0\n",
    "    best_v = 0\n",
    "    for v in tqdm(np.arange(1000) / 1000):\n",
    "        score = f1_score(oof_df[CFG.target_col], train_df[f'pred_prob'] >= v, average='macro')\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_v = v\n",
    "    print(best_score, best_v)\n",
    "    test_df['target'] = np.where(test_df['pred_prob'] >= best_v, 1, 0)\n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "#後処理の定義、調和平均版 \n",
    "def Postprocessing(train_df: pd.DataFrame(), test_df: pd.DataFrame()) -> (pd.DataFrame(), pd.DataFrame()): \n",
    "    train_df['pred_prob'] = 0 \n",
    "    weight_sum = 0 \n",
    "    for method in CFG.METHOD_LIST: \n",
    "        oof_df = pd.read_csv(f'oof_{method}_seed{CFG.seed}_ver{CFG.VER}.csv') \n",
    "        train_df['pred_prob'] += CFG.model_weight_dict[method] / oof_df[f'{method}_prediction'] \n",
    "        weight_sum += CFG.model_weight_dict[method] \n",
    "    train_df['pred_prob'] = weight_sum / train_df['pred_prob'] \n",
    "    best_score = 0 \n",
    "    best_v = 0 \n",
    "    for v in tqdm(np.arange(1000) / 1000):\n",
    "        score = f1_score(oof_df[CFG.target_col], train_df[f'pred_prob'] >= v, average='macro') \n",
    "        if score > best_score: \n",
    "            best_score = score \n",
    "            best_v = v \n",
    "    print(best_score, best_v) \n",
    "    test_df['target'] = np.where(test_df['pred_prob'] >= best_v, 1, 0)\n",
    "    return train_df, test_df\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 66,
     "referenced_widgets": [
      "ef58b4abcf0f43cd90a523a3875a2f28",
      "6f14799525cf44158bbffc0e9a08891b",
      "d239d2ff297a4c00aa350f8699ca6bb1",
      "76ddf1599bb144b2a6b4573027c6f662",
      "6762ef5aab7e4d18b98e7d837dedc03f",
      "b7dceec9910a46cea90915015ca58358",
      "573ab758539147f1a2bc21ac9f0347ce",
      "5de7c5cebd814e1caa8437775031ea12",
      "b0ab4a4e03b242e2ae3610b6a52b5c14",
      "4074a8c3b85548ed9424375a19416c7d",
      "13858cd0c554419bb867071b5c810b30"
     ]
    },
    "id": "q3P87swwPqf2",
    "outputId": "8d624fd1-4808-44f9-d59b-982a93d06ad5"
   },
   "outputs": [],
   "source": [
    "#後処理\n",
    "train_df, test_df = Postprocessing(train_df, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hK_3WqNDT0yj"
   },
   "outputs": [],
   "source": [
    "test_df[['target']].to_csv(f'seed{CFG.seed}_ver{CFG.VER}_{CFG.AUTHOR}_submission.csv', header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DHZryWelXTi4"
   },
   "source": [
    "特徴量の重要度を確認する方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 708
    },
    "id": "PpCipSr8gkD0",
    "outputId": "86d3f0b2-df9f-4dde-c13d-783497a759df"
   },
   "outputs": [],
   "source": [
    "model = pickle.load(open(f'lightgbm_fold1_seed42_ver{CFG.VER}.pkl', 'rb'))\n",
    "importance_df = pd.DataFrame(model.feature_importance(), index=features, columns=['importance'])\n",
    "importance_df['importance'] = importance_df['importance'] / np.sum(importance_df['importance'])\n",
    "importance_df.sort_values('importance', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# OOF予測を基に新たな特徴量を作成\n",
    "oof_features = np.zeros((train_df.shape[0], len(CFG.METHOD_LIST)))\n",
    "for i, method in enumerate(CFG.METHOD_LIST):\n",
    "    oof_df = pd.read_csv(f'oof_{method}_seed{CFG.seed}_ver{CFG.VER}.csv')\n",
    "    oof_features[:, i] = oof_df[f'{method}_prediction']\n",
    "\n",
    "# テストデータの予測を基に特徴量を作成\n",
    "test_features = np.zeros((test_df.shape[0], len(CFG.METHOD_LIST)))\n",
    "for i, method in enumerate(CFG.METHOD_LIST):\n",
    "    test_features[:, i] = test_df[f'{method}_pred_prob']\n",
    "\n",
    "# 特徴量の標準化\n",
    "scaler = StandardScaler()\n",
    "oof_features_scaled = scaler.fit_transform(oof_features)\n",
    "test_features_scaled = scaler.transform(test_features)\n",
    "\n",
    "# ロジスティック回帰モデルをパラメータチューニング・学習\n",
    "logistic = LogisticRegression()\n",
    "param_grid = {'C': [1]}\n",
    "grid_search = GridSearchCV(estimator=logistic, param_grid=param_grid, cv=5)\n",
    "grid_search.fit(oof_features_scaled, train_df[CFG.target_col])\n",
    "print('Best Parameter:',grid_search.best_params_)\n",
    "print('Best Score:',grid_search.best_score_)\n",
    "lr = LogisticRegression(C=grid_search.best_params_['C'])\n",
    "lr.fit(oof_features_scaled, train_df[CFG.target_col])\n",
    "\n",
    "# 最適な閾値とその時のF1スコアを探索する関数\n",
    "def find_best_threshold_and_score(y_true, y_pred_proba):\n",
    "    best_threshold = 0\n",
    "    best_score = 0\n",
    "    for threshold in np.linspace(0, 1, 1001):  # 0.001刻みで閾値を変更\n",
    "        score = f1_score(y_true, y_pred_proba >= threshold, average='macro')\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_threshold = threshold\n",
    "    return best_threshold, best_score\n",
    "\n",
    "# 学習データに対する予測確率\n",
    "train_pred_proba = lr.predict_proba(oof_features_scaled)[:, 1]\n",
    "\n",
    "# 最適な閾値とスコアを求める\n",
    "best_threshold, best_score = find_best_threshold_and_score(train_df[CFG.target_col], train_pred_proba)\n",
    "print(f'Best Threshold: {best_threshold}, Best F1 Score: {best_score}')\n",
    "\n",
    "# テストデータに対する最終予測\n",
    "test_pred_proba = lr.predict_proba(test_features_scaled)[:, 1]\n",
    "test_final_predictions = (test_pred_proba >= best_threshold).astype(int)\n",
    "\n",
    "# 最終予渲結果をコンペ提出用のフォーマットでCSVファイルに出力\n",
    "submission_df = pd.DataFrame({'Id': test_df.index, 'target': test_final_predictions}).reset_index(drop=True)\n",
    "# ここで、インデックスの開始が42307であるため、その値から始めるように調整\n",
    "submission_df['Id'] = submission_df.index + 42307\n",
    "\n",
    "submission_df.to_csv(f'stacking_lr_submission_best_score{best_score:.4f}_seed{CFG.seed}_ver{CFG.VER}_{CFG.AUTHOR}.csv', header=False, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "13858cd0c554419bb867071b5c810b30": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4074a8c3b85548ed9424375a19416c7d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "573ab758539147f1a2bc21ac9f0347ce": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5de7c5cebd814e1caa8437775031ea12": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6762ef5aab7e4d18b98e7d837dedc03f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6f14799525cf44158bbffc0e9a08891b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b7dceec9910a46cea90915015ca58358",
      "placeholder": "​",
      "style": "IPY_MODEL_573ab758539147f1a2bc21ac9f0347ce",
      "value": "100%"
     }
    },
    "76ddf1599bb144b2a6b4573027c6f662": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4074a8c3b85548ed9424375a19416c7d",
      "placeholder": "​",
      "style": "IPY_MODEL_13858cd0c554419bb867071b5c810b30",
      "value": " 1000/1000 [00:19&lt;00:00, 37.91it/s]"
     }
    },
    "b0ab4a4e03b242e2ae3610b6a52b5c14": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "b7dceec9910a46cea90915015ca58358": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d239d2ff297a4c00aa350f8699ca6bb1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5de7c5cebd814e1caa8437775031ea12",
      "max": 1000,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_b0ab4a4e03b242e2ae3610b6a52b5c14",
      "value": 1000
     }
    },
    "ef58b4abcf0f43cd90a523a3875a2f28": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_6f14799525cf44158bbffc0e9a08891b",
       "IPY_MODEL_d239d2ff297a4c00aa350f8699ca6bb1",
       "IPY_MODEL_76ddf1599bb144b2a6b4573027c6f662"
      ],
      "layout": "IPY_MODEL_6762ef5aab7e4d18b98e7d837dedc03f"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
